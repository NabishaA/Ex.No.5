# NAME : MONISH R
# REG NO : 212223220061
# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

# OUTPUT: 

## Explanation : 
Methodology:
Define Prompt Types:
### 1. Define the Two Prompt Types :
Naïve Prompt: Vague, open-ended, lacking specificity.

#### Example: "Tell me about space."

### Basic Prompt: Clear, structured, with explicit instructions.

#### Example: *"Explain the concept of black holes in 3-4 sentences, focusing on gravity and light absorption."*

### 2. Prepare Multiple Test Scenarios :
![Screenshot 2025-05-17 113508](https://github.com/user-attachments/assets/34ef0d6a-afad-45c2-bc8f-d2d94f6ac3bf)


### 3. Run Experiments with ChatGPT :
Input each naïve prompt, record the response.
Input the corresponding basic prompt, record the response.
Repeat for all scenarios.

### 4. Evaluate Responses :
Compare outputs based on:
 Quality (Clarity, coherence, engagement)
 Accuracy (Factual correctness)
 Depth (Detail, relevance, completeness)




#### Naive Prompts :
Broad, open-ended questions (e.g., "Tell me something interesting").

#### Basic Prompts :
Specific, structured instructions (e.g., "Explain the intuition behind AI prompts with two examples").

#### Select Test Scenarios :
Creative tasks (e.g., storytelling).
Factual queries (e.g., scientific explanations).
Summarization tasks.
Comparative analysis.

#### Execute Prompts :
Input both naive and basic prompts into ChatGPT and Gemini.
Record and label responses for comparison.

#### Evaluation Metrics :
Quality: Coherence, relevance, and engagement.
Accuracy: Factual correctness and logical consistency.
Depth: Detail, nuance, and adaptability to context.

## Results:
## 1. Intuition Behind Prompts

#### Naive Prompt : "Tell me something interesting."
 ChatGPT: Shared a fact about animal physiology.
 Gemini: Provided a fact about honey preservation.
 
 #### Basic Prompt: "Explain how AI prompts work with two examples."
ChatGPT: Defined prompts as contextual guides, offering translation and storytelling examples.
Gemini: Described prompts as predictors of word sequences, using sky color and translation examples.

## 2. Straightforward Queries

#### Naive Prompt: "Rain."
ChatGPT: Brief explanation of rain formation.
Gemini: Ultra-concise ("Water falling from clouds").

#### Basic Prompt: "Explain rainfall in 1–2 sentences."
Both models delivered clear, mechanistic explanations.

## 3. Tabular Data Requests
#### Naive Prompt: "List energy types."
ChatGPT: Bullet-pointed list.
Gemini: Categorized energy (kinetic/potential) with examples.

#### Basic Prompt: "Create a table of 3 energy types with definitions and examples."
Both models generated structured tables, though formatting differed slightly.

## 4. Comparative Analysis
#### Naive Prompt: "Compare things."
ChatGPT: Compared apples/oranges and cars/bicycles.
Gemini: Focused only on apples/oranges.

#### Basic Prompt: "Compare rule-based systems vs. machine learning."
Both models highlighted adaptability (ML) versus transparency (rule-based), with ChatGPT offering slightly more detail.

# Key Findings:
## Quality:

ChatGPT excelled in detailed, expansive responses.
Gemini prioritized brevity and precision.

## Accuracy:

Both models performed well, but ChatGPT handled complex queries more robustly.

## Depth:

Gemini provided nuanced answers to broad prompts.
ChatGPT thrived with structured prompts, delivering comprehensive outputs.

### Conclusion:
Structured prompts consistently yielded higher-quality, accurate, and detailed responses from both models.

#### Model Strengths:

ChatGPT: Ideal for users seeking elaboration and adaptability.

Gemini: Better for concise, focused answers.

### Results & Analysis : 

### 1. Creative Story Scenario :
![Screenshot 2025-05-17 113851](https://github.com/user-attachments/assets/c3ed4d7b-0d9c-490b-983b-157b9322b852)

### 2. Summarization (Climate Change) : 
![Screenshot 2025-05-17 113946](https://github.com/user-attachments/assets/8aab8d63-b6e1-4634-ba07-34c45131429c)

### 3.Comparative Analysis (Electric vs. Gasoline Cars) :
![Screenshot 2025-05-17 114040](https://github.com/user-attachments/assets/67e833fa-6e89-45da-93cb-7abae094575d)


# RESULT: 
The prompt for the above said problem executed successfully
